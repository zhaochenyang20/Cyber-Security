# 人工智能安全

## 人工智能安全应用

- 分类：网络信息安全应用，社会公共安全应用 P23

## 框架安全

### Pytorch 相比 TensorFlow 的优势

P35，设计简洁，易于理解，动态计算图

### **Keras** 和 Caffe

P36 ~ 39

### 框架安全漏洞

- TensorFlow：P41 ~ 43
- Pytorch、Keras 和 Caffe：P44 ~ 45

### **环境接触带来的漏洞**

1. 第三方基础库漏洞：P47 ~ 50
2. 可移植软件容器漏洞：P51 ~ 52

## 算法安全

### 鲁棒性安全

- 人工智能算法的优化原理 P62 ~ 63
- 人工智能算法的可解释性 P64 ~ 69

1. 建模前可解释性方法：数据可视化，寻找 ProtoTypes 和 Criticisms（典例与特例）——Prototype 是指能够表示大部分数据的数据实例，Criticism 是指不能由Prototype很好表示出的数据实例
2. 建立本身具备可解释性的模型：一些具有良好可解释性的模型包括决策树模型、线性模型以及贝叶斯实例模型等
3. 使用可解释性方法对模型进行解释：敏感性分析和隐层分析，基于可视化方法的模型解释（结构可视化、训练可视化）

- 人工智能算法的鲁棒性评估 P70

### 分类维度

P72 ~ 78

- 白盒攻击：攻击者能够获知机器学习所使用的算法，以及算法所使用的参数

- 黑盒攻击：只能通过输出输出与模型进行交互，迁移样本攻击（针对机器学习模型 A 构造的对抗性图像，也会有很大的比例能欺骗机器学习模型 B）

- 目标攻击：多分类问题被误分类为某个指定类别

- 无目标攻击：对抗样本最终属于哪一类

- 梯度攻击：考虑模型对样本的梯度，根据梯度的方向和大小等对样本进行调整，使损失函数增大

- 优化攻击：将输入样本视为可变量，并通过优化算法来最小化或最大化某个特定的目标函数，以找到最优的输入样本，使得模型在该样本上产生误导性的输出结果

### **前沿的攻击与防御**

1. 安全角度审视机器学习系统

机密性（Confidentiality）

- 模型隐私
- 数据隐私

完整性（Integrity）

- 数据投毒攻击（训练阶段）
- 对抗样本攻击（测试阶段）

可用性（Availability）

- 系统决策是否准确可靠

2. 投毒攻击

P82 ~ 83，设计攻击样本，混入到训练数据，让人工智能算法失效。

模型学习训练样本的分布，同时假设训练样本和测试样本是同分布的。如果扰乱训练数据的分布，自然会使模型对测试数据给出错误的输出。

P84：与对抗攻击区别：不改变目标机器学习系统，通过构造特定输入样本以完成欺骗目标系统。

P88 ~ 89：投毒攻击的防御，不能保证一个对已知攻击集有效的防御将不会对新的攻击失效。

## 算法局限性

1. 数据难以获取：小数据，假数据，孤岛数据（互补数据在现实中因为某种原因无法获得）
2. 数据不完整或偏斜
3. 训练集群资源限制、训练成本
4. 算法偏见：**词嵌入**层面就具有偏见
5. 伦理局限
